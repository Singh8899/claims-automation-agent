import json
from typing import Any, Dict, List

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from dotenv import find_dotenv, load_dotenv
from openai import OpenAI

# Load environment variables from .env file
load_dotenv(find_dotenv())

client = OpenAI()

def calculate_confusion_matrix(results: List[Dict[str, Any]]) -> Dict[str, Dict[str, int]]:
    decisions = ["APPROVE", "DENY", "UNCERTAIN"]
    matrix = {d: {d2: 0 for d2 in decisions} for d in decisions}
    
    for result in results:
        if 'error' in result:
            continue
        predicted = result.get('predicted_decision')
        expected = result.get('expected_decision')
        if predicted and expected and predicted in decisions and expected in decisions:
            matrix[expected][predicted] += 1
    
    return matrix


def generate_confusion_matrix_image(results: List[Dict[str, Any]], output_path: str = "confusion_matrix.png"):
    matrix_dict = calculate_confusion_matrix(results)
    decisions = ["APPROVE", "DENY", "UNCERTAIN"]
    
    matrix = np.array([[matrix_dict[exp][pred] for pred in decisions] for exp in decisions])
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues', 
                xticklabels=decisions, yticklabels=decisions,
                cbar_kws={'label': 'Count'}, linewidths=1, linecolor='gray')
    
    plt.xlabel('Predicted Decision', fontsize=12, fontweight='bold')
    plt.ylabel('Expected Decision', fontsize=12, fontweight='bold')
    plt.title('Confusion Matrix - Claims Processing Evaluation', fontsize=14, fontweight='bold', pad=20)
    
    total = matrix.sum()
    correct = np.trace(matrix)
    accuracy = (correct / total * 100) if total > 0 else 0
    plt.text(0.5, -0.15, f'Overall Accuracy: {accuracy:.2f}% ({correct}/{total})', 
             transform=plt.gca().transAxes, ha='center', fontsize=11, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()



SYSTEM_PROMPT_EXPLANATION_JUDGE = """
You are an expert judge evaluating whether a predicted explanation for a claim decision matches or aligns with the expected explanation.

Your task is to determine if the predicted explanation captures the same core reasoning as the expected explanation, even if worded differently.

EVALUATION CRITERIA:

1. **MATCH (score: 1.0):**
   - Predicted explanation identifies the same core issue(s) as expected
   - May use different wording but reasoning is equivalent
   - All key points from expected explanation are covered

2. **PARTIAL MATCH (score: 0.5):**
   - Predicted explanation identifies some but not all key issues
   - Correct reasoning but incomplete coverage
   - Additional correct points mentioned beyond expected

3. **NO MATCH (score: 0.0):**
   - Predicted explanation identifies different issues than expected
   - Incorrect or irrelevant reasoning
   - Missing all key points from expected explanation

4. **NO EXPECTED EXPLANATION (score: null):**
   - If expected explanation is empty, null, or not provided, return null

OUTPUT FORMAT:
Respond with ONLY a single JSON object:
{
  "score": <1.0, 0.5, 0.0, or null>,
  "reasoning": "<brief explanation of why this score was assigned>"
}

Be objective and focus on semantic equivalence, not exact wording.
"""


def evaluate_explanation_match(predicted_explanation: str, expected_explanation: str) -> dict:
    """
    Use LLM-as-judge to evaluate if predicted explanation matches expected explanation.
    
    Args:
        predicted_explanation: The explanation generated by the agent
        expected_explanation: The expected explanation from the test set
    
    Returns:
        dict with 'score' (float or None) and 'reasoning' (str)
    """
    if not expected_explanation or expected_explanation.strip() == "":
        return {"score": None, "reasoning": "No expected explanation provided"}
    
    query = f"""Evaluate the following claim decision explanations:

            EXPECTED EXPLANATION:
            {expected_explanation}

            PREDICTED EXPLANATION:
            {predicted_explanation}

            Do these explanations match in their core reasoning? Provide your evaluation as a JSON object.
    """
    
    try:
        response = client.responses.create(
            model="gpt-5-mini",
            instructions=SYSTEM_PROMPT_EXPLANATION_JUDGE,
            input=[{
                "role": "user",
                "content": [{"type": "input_text", "text": query}],
            }],
            reasoning={"effort": "low"},
            text={"verbosity": "low"},
        )
        result = json.loads(response.output_text)
        return result
    except Exception as e:
        return {"score": None, "reasoning": f"Error evaluating explanation: {str(e)}"}
